{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Task (XOR Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook a classification task based on the XOR (www. function is presented. \n",
    "\n",
    "\n",
    "#### The procedure is the following:\n",
    "* Generate a data set containing two classes which are not linearly separable\n",
    "* By the variable 'center_gap' the overlap of the two classes can be modified\n",
    "* Visualize sampled data set\n",
    "* Train a linear classifier and evaluate the results\n",
    "* Train a non-linear classifier and evaluate the results\n",
    "\n",
    "\n",
    "#### Self-study [20min]:\n",
    "* Go through notebook on your own and work on tasks presented in the end of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "from res.plot_lib import set_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default()\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Pytorch Device: GPU if available, else use CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Notebook Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_gap = 6         # Distance between cluster centers of the data set\n",
    "num_samples = 2500     # Number of samples included in the data set\n",
    "num_of_classes = 2     # Number of classes in the data set\n",
    "val_left_out = 0.2     # Fraction of data set used for validation\n",
    "\n",
    "H = 10                 # Number of hidden unites in the Neural Netowrk\n",
    "num_out = 1            # Number of outputs in the neural network (Binary classifier needs only one output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerate XOR Data Set with Four Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from 2 dimensinal normal distribution\n",
    "X = torch.normal(mean=0, std=1, size=(num_samples,2))\n",
    "\n",
    "# Sample cluster ids randomly from {0,1,2,3}\n",
    "cluster_id = torch.randint(low=0,high=4, size=(num_samples,))\n",
    "\n",
    "# Transform data point to receive four clusters\n",
    "#      odd cluster_id   -->  shift x value by center_gap\n",
    "#      cluster_id >= 2  -->  shift y value by center_gap\n",
    "X = torch.stack([X[:,0] + center_gap * (cluster_id % 2), \n",
    "                 X[:,1] + center_gap * (cluster_id / 2)], axis=-1)\n",
    "\n",
    "# Map cluster_ids to class labels: \n",
    "#      cluster_ids 0 and 3 --> 0\n",
    "#      cluster_ids 1 and 4 --> 1\n",
    "Y = torch.where((cluster_id == 0) + (cluster_id == 3) > 0, \n",
    "                torch.zeros_like(cluster_id), \n",
    "                torch.ones_like(cluster_id)).type(torch.FloatTensor)\n",
    "Y = torch.unsqueeze(Y, axis=-1)\n",
    "\n",
    "\n",
    "# Split data into training and validation set\n",
    "split_id = int(num_samples*val_left_out)\n",
    "cluster_id_train = cluster_id[split_id:]\n",
    "X_train = X[split_id:]\n",
    "Y_train = Y[split_id:]\n",
    "cluster_id_val = cluster_id[:split_id]\n",
    "X_val = X[:split_id]\n",
    "Y_val = Y[:split_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Data Set Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shapes:\")\n",
    "print(\"X_train:\", tuple(X_train.size()))\n",
    "print(\"Y_train:\", tuple(Y_train.size()))\n",
    "print(\"Cluster Ids Training:\", torch.unique(cluster_id_train,return_counts=True))\n",
    "print(\"X_val:  \", tuple(X_val.size()))\n",
    "print(\"Y_val:  \", tuple(Y_val.size()))\n",
    "print(\"Cluster Ids Validation:\", torch.unique(cluster_id_val,return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 7))\n",
    "\n",
    "# Plot Training Data\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.scatter(X_train.cpu()[Y_train[:,0]==0,0].numpy(), X_train.cpu()[Y_train[:,0]==0,1].numpy(), color=\"green\", label=\"Class 1\")\n",
    "plt.scatter(X_train.cpu()[Y_train[:,0]==1,0].numpy(), X_train.cpu()[Y_train[:,0]==1,1].numpy(), color=\"yellow\", label=\"Class 2\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.axis('equal');\n",
    "plt.title('Training Data');\n",
    "\n",
    "# Plot Validation Data\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.scatter(X_val.cpu()[Y_val[:,0]==0,0].numpy(), X_val.cpu()[Y_val[:,0]==0,1].numpy(), color=\"green\", label=\"Class 1\")\n",
    "plt.scatter(X_val.cpu()[Y_val[:,0]==1,0].numpy(), X_val.cpu()[Y_val[:,0]==1,1].numpy(), color=\"yellow\", label=\"Class 2\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.axis('equal');\n",
    "plt.title('Validation Data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate Classifiers\n",
    "\n",
    "We now build, train and evaluate a linear classifier and a non-linear neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "* Train ``num_networks`` networks \n",
    "* Use the binary-cross-entropy as loss function ``torch.nn.BCELoss()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "max_epochs  = 500                # Maximum number of epochs to train\n",
    "num_networks = 5                 # Number of networks to be trained\n",
    "models = []                      # Empty list to be filled with trained models\n",
    "criterion = torch.nn.BCELoss()   # Use binary-cross-entropy as loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classifier\n",
    "\n",
    "For the linear classifier, we train 5 networks and compare the resulting predictions by visualization of the predictions on the validation data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Neural Network without Non-Linearities\n",
    "* Use nn package to create our linear model\n",
    "* The network consists of H hidden units\n",
    "* Each Linear module has a weight and bias\n",
    "* Apply sigmoid on output to receive a probability value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_linear():\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(2, H),\n",
    "        nn.Linear(H, 1),\n",
    "        nn.Sigmoid()      # Sigmoid function to receive probabilities\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(build_model_linear())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Neural Network without Non-Linearities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = []\n",
    " \n",
    "# Iterate through number of networks\n",
    "for n in range(num_networks):\n",
    "    torch.manual_seed(seed + n);\n",
    "    model = build_model_linear()\n",
    "    model.to(device)       # move model to device\n",
    "    models.append(model)\n",
    "\n",
    "    # we use the optim package to apply stochastic gradient descent for our parameter updates\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)   \n",
    "    \n",
    "    for t in range(max_epochs):\n",
    "\n",
    "        # Feed forward to get prediction\n",
    "        y_pred_prob = model(X_train)\n",
    "\n",
    "        # Compute the loss (Binary-Cross-Entropy)\n",
    "        loss = criterion(y_pred_prob, Y_train)\n",
    "\n",
    "        # Print current progress\n",
    "        print(\"[MODEL]: %i, [EPOCH]: %i, [LOSS]: %.6f\" % (n+1, t, loss.item()))\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "        # zero the gradients before running the backward pass.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass to compute the gradient of loss w.r.t our learnable params. \n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Predictions of Validation Set for each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 7))\n",
    "rows = 2\n",
    "columns = 5\n",
    "\n",
    "for i, m in enumerate(models):\n",
    "    fig.add_subplot(rows, columns, i+1)\n",
    "    \n",
    "    y_pred_prob = m(X_val)\n",
    "    y_pred = torch.round(y_pred_prob)\n",
    "\n",
    "    plt.scatter(X_val.cpu()[y_pred[:,0]==1, 0].numpy(), X_val.cpu()[y_pred[:,0]==1, 1].numpy(), color=\"green\")\n",
    "    plt.scatter(X_val.cpu()[y_pred[:,0]==0, 0].numpy(), X_val.cpu()[y_pred[:,0]==0, 1].numpy(), color=\"yellow\")\n",
    "    plt.axis('equal')\n",
    "    plt.title('Model %i \\n Loss: %f' %(i+1, criterion(y_pred_prob, Y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercises\n",
    "* Reduce the number of training samples and retrain the model. What do you expect to happen? What can you see?\n",
    "* Reduce the gap between the centers. The overlapping of the classes increases. What happens to the variety of the trained models? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Two-Layered Non-Linear Network\n",
    "\n",
    "In this excercise you extend the above presented examples to a non-linear classifier. This can be realized by adding different non-linearities to the model description. \n",
    "\n",
    "Go through the code below and fill the missing parts by parts (marked as '???') such that \n",
    "* the code trains 5 different networks\n",
    "* add the Tanh non-linearity (nn.Tanh()) to the network architecture\n",
    "* \n",
    "\n",
    "\n",
    "\n",
    "For the non-linear classifier, we train 5 networks and compare the resulting predictions by visualization of the predictions on the validation data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-1\n",
    "max_epochs  = 500                # Maximum number of epochs to train\n",
    "num_networks = ???               # Number of networks to be trained\n",
    "models = []                      # Empty list to be filled with trained models\n",
    "criterion = torch.nn.BCELoss()   # Use binary-cross-entropy as loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Neural Network with Non-Linearities\n",
    "* Use nn package to create our linear model\n",
    "* The network consists of H hidden units\n",
    "* Each Linear module has a weight and bias\n",
    "* Apply sigmoid on output to receive a probability value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(2, H),\n",
    "        ???\n",
    "        nn.Linear(H, H),\n",
    "        ???\n",
    "        nn.Linear(H, 1),\n",
    "        nn.Sigmoid()      # Sigmoid function to receive probabilities\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(build_model())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Neural Network with Non-Linearities\n",
    "* Train 'num_networks' networks \n",
    "* Use the binary-cross-entropy as loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "# Iterate through number of networks\n",
    "for n in range(num_networks):\n",
    "    \n",
    "    torch.manual_seed(seed + n);\n",
    "    model = build_model()\n",
    "    model.to(device)       # move model to device\n",
    "    models.append(model)\n",
    "\n",
    "    # we use the optim package to apply\n",
    "    # stochastic gradient descent for our parameter updates\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)   \n",
    "    \n",
    "    for t in range(max_epochs):\n",
    "\n",
    "        # Feed forward to get prediction\n",
    "        y_pred_prob = model(X_train)\n",
    "\n",
    "        # Compute the loss (Binary-Cross-Entropy)\n",
    "        loss = criterion(y_pred_prob, Y_train)\n",
    "\n",
    "        # Print current progress\n",
    "        print(\"[MODEL]: %i, [EPOCH]: %i, [LOSS]: %.6f\" % (n+1, t, loss.item()))\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "        # zero the gradients before running the backward pass.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass to compute the gradient of loss w.r.t our learnable params. \n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Predictions of Validation Set for each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(models, X_val):\n",
    "    fig = plt.figure(figsize=(15, 7))\n",
    "    rows =2\n",
    "    columns = 5\n",
    "\n",
    "    for i, m in enumerate(models):\n",
    "        fig.add_subplot(rows, columns, i+1)\n",
    "    \n",
    "        y_pred = torch.round(m(X_val))\n",
    "\n",
    "        plt.scatter(X_val.cpu()[y_pred[:,0]==1, 0].numpy(), X_val.cpu()[y_pred[:,0]==1, 1].numpy(), color=\"green\")\n",
    "        plt.scatter(X_val.cpu()[y_pred[:,0]==0, 0].numpy(), X_val.cpu()[y_pred[:,0]==0, 1].numpy(), color=\"yellow\")\n",
    "        plt.axis('equal')\n",
    "        plt.title('Model %i' %(i+1))\n",
    "        \n",
    "predict_and_plot(models, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_gap = 6         # Distance between cluster centers of the data set\n",
    "num_samples = 2500     # Number of samples included in the data set\n",
    "num_of_classes = 2     # Number of classes in the data set\n",
    "val_left_out = 0.2     # Fraction of data set used for validation\n",
    "\n",
    "H = 10                 # Number of hidden unites in the Neural Netowrk\n",
    "num_out = 1            # Number of outputs in the neural network (Binary classifier needs only one output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shapes:\")\n",
    "print(\"X_train:\", tuple(X_train.size()))\n",
    "print(\"Y_train:\", tuple(Y_train.size()))\n",
    "print(\"Cluster Ids Training:\", torch.unique(cluster_id_train,return_counts=True))\n",
    "print(\"X_val:  \", tuple(X_val.size()))\n",
    "print(\"Y_val:  \", tuple(Y_val.size()))\n",
    "print(\"Cluster Ids Validation:\", torch.unique(cluster_id_val,return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
