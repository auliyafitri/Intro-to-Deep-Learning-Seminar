{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification\n",
    "\n",
    "In the final part we will use what we learned earlier to classify images. \n",
    "\n",
    "In a first section we will go through the basics of image pre-processing and how to classify images using a fully connected network.\n",
    "\n",
    "In the second section you will use a more sophisticated approach, namely a Convolutional Neural Network (CNN) and discover it's advantages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we start, load some libraries ...\n",
    "from res.plot_lib import plot_data, plot_model, set_default\n",
    "set_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... and some more\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import numpy\n",
    "\n",
    "# helper function to count number of parameters\n",
    "def get_n_params(model):\n",
    "    np=0\n",
    "    for p in list(model.parameters()):\n",
    "        np += p.nelement()\n",
    "    return np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running on a GPU: device string\n",
    "Switching between CPU and GPU in PyTorch is controlled via a device string, which will seemlessly determine whether GPU is available, falling back to CPU if not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Dataset (MNIST)\n",
    "\n",
    "MNIST is a famous image classifiaction dataset, consisting of grayscale images of handwritten digits.\n",
    "\n",
    "We can use some PyTorch DataLoader utilities for this. This will download, shuffle, normalize data and arrange it in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_size  = 28*28   # images are 28x28 pixels\n",
    "output_size = 10      # there are 10 classes\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data/', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=64, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data/', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1600x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show some images\n",
    "plt.figure(figsize=(16, 6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    image, _ = train_loader.dataset.__getitem__(i)\n",
    "    plt.imshow(image.squeeze().numpy())\n",
    "    plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training and Evaluation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list = []\n",
    "\n",
    "def train(epoch, model, perm=torch.arange(0, 784).long()):\n",
    "    dh = display.display(None, display_id=True)\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # send to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # permute pixels\n",
    "        data = data.view(-1, 28*28)\n",
    "        data = data[:, perm]\n",
    "        data = data.view(-1, 1, 28, 28)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # pass data through the model\n",
    "        output = model(data)\n",
    "        # calculate loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # calculate gradients of the loss wrt. to model parameters\n",
    "        loss.backward()\n",
    "        # and update parameters accordingly\n",
    "        optimizer.step()\n",
    "        \n",
    "        # output training status\n",
    "        if batch_idx % 10 == 0:\n",
    "            dh.update(f\"[MODEL]: {model.__class__.__name__}, [EPOCH]: {epoch}, \"\n",
    "                    + f\"[BATCH]: {batch_idx}, [LOSS]: {loss.item():.6f}\")\n",
    "            \n",
    "            \n",
    "def test(model, perm=torch.arange(0, 784).long()):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        # send to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # permute pixels\n",
    "        data = data.view(-1, 28*28)\n",
    "        data = data[:, perm]\n",
    "        data = data.view(-1, 1, 28, 28)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss                                                               \n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the prediction                                                                 \n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    accuracy_list.append(accuracy)\n",
    "    \n",
    "    print(f\"Test set: Average loss: {test_loss:.4f}, \"\n",
    "          + f\"Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification using a Fully Connected Network\n",
    "\n",
    "In this section we will define a Fully Connected Network and train it on a chunk of the MNIST dataset. After it has been trained we will use it to classify the remaining images, the network has not \"seen\" before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Define a Fully Conected Network\n",
    "\n",
    "A Fully Connected Layer (aka Linear Layer) is defined by\n",
    "\n",
    "```fc = torch.nn.Linear(in_features, out_features, bias=True)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    def __init__(self, input_size, n_features, output_size):\n",
    "        super(FCN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Flatten(), # this turns the 28x28 input image into a 784-dim vector \n",
    "            nn.Linear(in_features=input_size, out_features=n_features), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(n_features, n_features), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(n_features, output_size), \n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train the Fully Connected Network on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 6442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[MODEL]: FCN, [EPOCH]: 1, [BATCH]: 930, [LOSS]: 0.450249'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.3922, Accuracy: 8846/10000 (88%)\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 8 # number of hidden units\n",
    "\n",
    "model_fnn = FCN(input_size, n_hidden, output_size)\n",
    "model_fnn.to(device)\n",
    "optimizer = optim.SGD(model_fnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_fnn)))\n",
    "\n",
    "for epoch in range(1, 2):\n",
    "    train(epoch, model_fnn)\n",
    "    test(model_fnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Deep Learning\n",
    "* Many layers: compositionality\n",
    "* Convolutions: locality + stationarity of images\n",
    "* Pooling: Invariance of object class to translations\n",
    "\n",
    "Let's use a Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Excercise: Classification using a Convolutional Neural Network (CNN)\n",
    "\n",
    "* Convolutions allow us to extract prior knowledge about the images (Locality and stationarity)\n",
    "* Pooling builds in some translation invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 How to build a CNN\n",
    "\n",
    "A 2D Convolutional layer is defined by:\n",
    "\n",
    "```python\n",
    "nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "```\n",
    "(for the full argument list see: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)\n",
    "\n",
    "A 2D MaxPooling layer is defined by:\n",
    "\n",
    "```python\n",
    "nn.MaxPool2d(kernel_size)\n",
    "```\n",
    "(for the full argument list see: https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html)\n",
    "\n",
    "A typical convolutional block consists of a convolutional layer followed by a nonlinearity (e.g. `nn.ReLU()`) and a pooling layer.\n",
    "\n",
    "To connect a convolutinal layer (2-dim) to a fully connected layer (1-dim), we need to flatten the output of the convoltional layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Hands-On Excercise\n",
    "\n",
    "1. Define a Convolutional Neural Network, consisting of 2 convolutional blocks and 2 fully connected layers. (Don't forget to Softmax)\n",
    "  * Hint: The input_size of the first Fully Connected Layer is `n_features * 4 * 4`\n",
    "2. Train your CNN on the MNIST dataset and compare the result to the FCN. \n",
    "  * Does it perform better? \n",
    "  * If it does, why?\n",
    "3. Optional: What happens if you ask your CNN to classify a \"rotated 1\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excercise 1: Define CNN\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, n_features, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=n_features, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(n_features, n_features, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(n_features*4*4, 50), \n",
    "            nn.Linear(50, output_size),  \n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 6422\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[MODEL]: CNN, [EPOCH]: 1, [BATCH]: 930, [LOSS]: 0.140655'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.1449, Accuracy: 9537/10000 (95%)\n"
     ]
    }
   ],
   "source": [
    "# Excercise 2: Training and Evaluation\n",
    "\n",
    "n_features = 6 # number of feature maps. \n",
    "# Note: 6 is chosen in order to have similar number of parameters from FCN \n",
    "# so we can compare the performace of both networks given same number of parameters.\n",
    "# However, you can choose any number of feature\n",
    "\n",
    "model_cnn = CNN(input_size, n_features, output_size)\n",
    "model_cnn.to(device)\n",
    "optimizer = optim.SGD(model_cnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_cnn)))\n",
    "\n",
    "for epoch in range(1, 2):\n",
    "    train(epoch, model_cnn)\n",
    "    test(model_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excercise 3: Pass a rotated 1 through your CNN. \n",
    "\n",
    "# You can use the following functions:\n",
    "def gimmeOne():\n",
    "    i = 0\n",
    "    for data, target in test_loader.dataset:\n",
    "        if target == 1:\n",
    "            return data, target\n",
    "\n",
    "def show_image(image):\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(image.squeeze().numpy())\n",
    "    plt.axis('off');\n",
    "\n",
    "def predict_single_image(model, image):\n",
    "    model_cnn.eval()\n",
    "    perm=torch.arange(0, 784).long()\n",
    "    \n",
    "    image = image.view(-1, 28*28)\n",
    "    image = image[:, perm]\n",
    "    image = image.view(-1, 1, 28, 28)\n",
    "    \n",
    "    prediction = model_cnn(image)\n",
    "    return torch.argmax(prediction)\n",
    "\n",
    "def rotate_image(image, angle):\n",
    "    rot = transforms.RandomRotation((angle, angle))\n",
    "    return rot(image)\n",
    "\n",
    "# put your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1, Target: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPYAAAD2CAYAAADlEnrIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAFI0lEQVR4nO3dX6jecxzA8d+x2WZpYVH+LBGpzbTyt4j8LYUr966kxLV7aldyQ1LihrgwZQo3LkRZqSEhcoNwITET27LtuJCLJd9nxzk723nv9br9POf3fOvp3efi+/ScuWma5icg5ZTjfQBg6QkbgoQNQcKGIGFDkLAhSNgQJGwIEjYErT7aF942d++xPAdwlN6e3zHzNTY2BAkbgoQNQcKGIGFDkLAhSNgQJGwIEjYECRuChA1BwoYgYUOQsCFI2BAkbAgSNgQJG4KEDUHChiBhQ5CwIUjYECRsCBI2BAkbgoQNQcKGIGFDkLAh6Kj/jS5M0zTNXbllOH/j9ReG863PPDScb3rs/QWfiX+zsSFI2BAkbAgSNgQJG4KEDUHChiBhQ5AvqLAgP169YTg/OB0aztf/ML+Ux+E/2NgQJGwIEjYECRuChA1BwoYgYUOQe2wW5JcrxvfU3x08MJxvfG7XUh6H/2BjQ5CwIUjYECRsCBI2BAkbgoQNQe6xOcL89duG8/fuemI4v+ndh4fzS6aPFnwmFs7GhiBhQ5CwIUjYECRsCBI2BAkbgtxjc4SfN582nJ+7av1wfv6OU5fyOPxPNjYECRuChA1BwoYgYUOQsCFI2BDkHpsj3Prg+He/X/v9jOH89He+HM7Hv0rOUrGxIUjYECRsCBI2BAkbgoQNQcKGIPfYJ5FVWy6b+Zrt57w8nD+394Lh/NCeXxd0Jo4NGxuChA1BwoYgYUOQsCFI2BAkbAhyj30S+f72jYt+xu7fLpzxin2Lfg8Wz8aGIGFDkLAhSNgQJGwIEjYECRuC3GOfRPZu/nPRz/j4qW3D+RnT+HfJWR42NgQJG4KEDUHChiBhQ5CwIUjYEOQeO+TAnVcP5zvveHLmMx796crh/KxXPxnOD898B5aDjQ1BwoYgYUOQsCFI2BAkbAgSNgS5xw757pbxx3nFmnUzn3Hf11uH83N+/2JBZ+L4sLEhSNgQJGwIEjYECRuChA1BwoYgYUOQL6iEnH35j8P5ofnZP4OweueZS3UcjiMbG4KEDUHChiBhQ5CwIUjYECRsCHKPvYKsvujC4fzxy14Zzp/9ddPM9zjref+4vsDGhiBhQ5CwIUjYECRsCBI2BAkbgtxjryBfPXDecH7d2vHf3//hzTPfY9P06UKOxAnKxoYgYUOQsCFI2BAkbAgSNgQJG4LcY68ghzftX9Tf79sz+x/f02BjQ5CwIUjYECRsCBI2BAkbgoQNQe6xV5Cnr31xUX9//lurlugknOhsbAgSNgQJG4KEDUHChiBhQ5CwIcg99glk/93XDOc3rPtgxhN8nPzNxoYgYUOQsCFI2BAkbAgSNgQJG4JcfJ5Avr1nfjhfOzf+uB79aetwfvrO3TPPMD4BK4WNDUHChiBhQ5CwIUjYECRsCBI2BLnHXkarNmwYzh+5/s1FPf+lt24czi8+uGtRz2flsLEhSNgQJGwIEjYECRuChA1BwoYg99jL6PCBA8P553+cN5zf9v1Vw/ml2z8bzg8Np5TY2BAkbAgSNgQJG4KEDUHChiBhQ5CwIcgXVJbR/IwvqHw5/v7JtGb6Zjj3BRT+YWNDkLAhSNgQJGwIEjYECRuChA1BwoYgYUOQsCFI2BAkbAgSNgQJG4KEDUHChiBhQ5CwIUjYECRsCBI2BAkbgoQNQcKGIGFDkLAhSNgQJGwIEjYECRuChA1Bc9M0zR/vQwBLy8aGIGFDkLAhSNgQJGwIEjYECRuChA1BwoagvwBGqG/wsSjlQwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get a one from the test set and predict it\n",
    "image, target = gimmeOne()\n",
    "show_image(image)\n",
    "prediction = predict_single_image(model_cnn, image)\n",
    "print(f\"Prediction: {prediction}, Target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotate the image and predict again\n",
    "img_rotated = rotate_image(image, 90)\n",
    "show_image(img_rotated)\n",
    "prediction = predict_single_image(model_cnn, img_rotated)\n",
    "print(f\"Prediction: {prediction}, Target: {target}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
